<!-- content.md -->
ðŸ“˜ Topic: Algorithm Analysis (Complexities)

ðŸ§  Overview  
Algorithm analysis is the foundation for evaluating how efficient an algorithm is in terms of time and space as the input size grows. We use **asymptotic notations**â€”Big-O, Big-Î˜, and Big-Î©â€”to describe upper bounds, tight bounds, and lower bounds on an algorithmâ€™s running time (or space usage). Understanding these helps you choose the right algorithm for large inputs and guide optimizations.

â± Time Complexity  

| Name             | Notation       | Growth Rate          | Example                      |
|------------------|----------------|----------------------|------------------------------|
| Constant         | O(1)           | 1                    | Accessing an array element  |
| Logarithmic      | O(log n)       | log n                | Binary Search               |
| Linear           | O(n)           | n                    | Single loop over an array   |
| Linearithmic     | O(n log n)     | n log n              | Merge Sort, Quick Sort (avg)|
| Quadratic        | O(nÂ²)          | nÂ²                   | Simple nested loops         |
| Exponential      | O(2â¿)          | 2â¿                   | Recursive Tower of Hanoi    |

> **Best-case, Average-case, Worst-case**  
> â€¢ *Best-case* (Î©) gives a lower bound (e.g., O(1) for binary search if the target is at the middle).  
> â€¢ *Average-case* often assumes a random distribution of inputs.  
> â€¢ *Worst-case* (O) is most critical for guarantees (e.g., quicksortâ€™s O(nÂ²) when pivot is always worst).

---

### ðŸ”¹ Optimized Idea  
To illustrate optimization, compare **linear search** (O(n)) vs **binary search** (O(log n)) on a sorted array:
1. **Linear search** scans each element until it finds the target.  
2. **Binary search** repeatedly splits the search interval in half.

By switching to binary search whenever the data is sorted, you reduce growth from linear to logarithmicâ€”hugely significant for large n (e.g. n = 1 000 000 â‡’ 20 steps vs 1 000 000 steps).

---

### ðŸ”¹ Code Fragment  
```python
def binary_search(arr, target):
    lo, hi = 0, len(arr) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if arr[mid] == target:
            return mid        # O(1) access
        elif arr[mid] < target:
            lo = mid + 1     # halves the search space
        else:
            hi = mid - 1
    return -1
# Overall time complexity: O(log n)
```

---

### ðŸ”¹ Binary Search
```python
def binary_search(arr, target):
    lo, hi = 0, len(arr) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            lo = mid + 1
        else:
            hi = mid - 1
    return -1
# Worst-case: O(log n)
```

Switching from linear to binary search on sorted data reduces steps from n to log nâ€”critical for large n.

---

### ðŸ”¹ Space Complexity
Measures extra memory usage. \
â€¢ In-place algorithms use O(1) extra space (e.g., in-place quicksort). \
â€¢ Not in-place may require O(n) auxiliary space (e.g., mergesortâ€™s merge).

---

### ðŸ”¹ Amortized Analysis
Smooths out occasional expensive operations by averaging over a sequence. \
â€¢ Dynamic array resizing: occasional O(n) copy, but amortized O(1) per insert.

---

### ðŸ”¹ Master Theorem
For recurrences of form T(n) = a T(n/b) + f(n), compare f(n) to n^{log_b(a)}:

1. If f(n)=O(n^{log_b(a)âˆ’Îµ}), T(n)=Î˜(n^{log_b(a)})
2. If f(n)=Î˜(n^{log_b(a)}Â·log^k n), T(n)=Î˜(n^{log_b(a)}Â·log^{k+1} n)
3. If f(n)=Î©(n^{log_b(a)+Îµ}) and regularity holds, T(n)=Î˜(f(n))

> Example: Merge Sort \
> T(n)=2 T(n/2)+Î˜(n) â‡’ case 2 â‡’ Î˜(n log n)
